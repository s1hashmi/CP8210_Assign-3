library(readr)
processed_cleveland <- read_csv("S:/Ryerson/Masters/W22/Topics in DS - CP8210/Assing/2/processed_cleveland.csv")
View(processed_cleveland)
source("~/.active-rstudio-document")
data <- processed_cleveland
View(data)
View(data)
as.factor(data$X3)
as.factor(data$X2)
as.factor(data$X6)
as.factor(data$X3)
as.factor(data$X7)
as.factor(data$X9)
as.factor(data$X11)
as.factor(data$X13)
as.factor(data$X14)
as.factor(data$X12)
set.seed(500)
# any missing points
apply(data,2,function(x) sum(is.na(x)))
library(tidyverse)  #tidyverse for easy data manipulation
library("tidyverse")  #tidyverse for easy data manipulation
library(tidyr)
library(caret)
library(nnet, lib.loc = "C:/Program Files/R/R-4.1.2/library")
library(jmvconnect)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
mlog.fit <- glm(X14~., data=train, family="binomial")
summary(mlog.fit)
pr.mlog <- predict(mlog.fit,test)
MSE.mlog <- sum((pr.mlog - test$medv)^2)/nrow(test)
MSE.mlog <- sum((pr.mlog - test$X14)^2)/nrow(test)
#normalization of dataset
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
install.packages("neuralnet")
# building the net
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("X14 ~", paste(n[!n %in% "X14"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden= 10,linear.output=T)
# plot the net
plot(nn)
# try to predict the values for the test set and calculate the MSE, we have 13 inputs
pr.nn <- compute(nn,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$X14)-min(data$X14))+min(data$X14)
test.r <- (test_$X14)*(max(data$X14)-min(data$X14))+min(data$X14)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
#we then compare the two MSEs
print(paste(MSE.mlog,MSE.nn))
par(mfrow=c(1,2))
plot(test$X14,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$X14,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
plot(test$X14,pr.mlog,col='blue',main='Real vs predicted mlog',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='MLog',pch=18,col='blue', bty='n', cex=.95)
n <- names(train_)
f <- as.formula(paste("X14 ~", paste(n[!n %in% "X14"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden= 10,act.fct = "logistic",
linear.output = FALSE)
# plot the net
plot(nn)
# try to predict the values for the test set and calculate the MSE, we have 13 inputs
pr.nn <- compute(nn,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(data$X14)-min(data$X14))+min(data$X14)
test.r <- (test_$X14)*(max(data$X14)-min(data$X14))+min(data$X14)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
#we then compare the two MSEs
print(paste(MSE.mlog,MSE.nn))
library(stargazer)
stargazer(nn, mlog.fit, type = "text")
stargazer(nn,mlog.fit, type = "text")
stargazer(nn,mlog.fit)
par(mfrow=c(1,2))
plot(test$X14,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$X14,pr.mlog,col='blue',main='Real vs predicted mlog',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='MLog',pch=18,col='blue', bty='n', cex=.95)
# Both on same graph
plot(test$X14,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$X14,pr.mlog,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','MLog'),pch=18,col=c('red','blue'))
install.packages(boot)
install.packages("boot")
library(boot)
set.seed(200)
mlog.fit <- glm(X14~., binomial, data=data)
cv.glm(data,mlog.fit,K=10)$delta[1]
install.packages("plyr")
install.packages("plyr")
cv.error <- NULL
k <- 10
library(plyr)
pbar <- create_progress_bar('text')
pbar$init(k)
for(i in 1:k){
index <- sample(1:nrow(data),round(0.9*nrow(data)))
train.cv <- scaled[index,]
test.cv <- scaled[-index,]
nn <- neuralnet(f,data=train.cv,hidden= 10,act.fct = "logistic",
linear.output = FALSE)
pr.nn <- compute(nn,test.cv[,1:13])
pr.nn <- pr.nn$net.result*(max(data$X14)-min(data$X14))+min(data$X14)
test.cv.r <- (test.cv$X14)*(max(data$X14)-min(data$X14))+min(data$X14)
cv.error[i] <- sum((test.cv.r - pr.nn)^2)/nrow(test.cv)
pbar$step()
}
#calculate the average MSE and plot the results as a boxplot
mean(cv.error)
cv.error
boxplot(cv.error,xlab='MSE CV',col='cyan',
border='blue',names='CV error (MSE)',
main='CV error (MSE) for NN',horizontal=TRUE)
boxplot(cv.error,xlab='MSE CV',col='cyan',
border='blue',names='CV error (MSE)',
main='CV error (MSE) for NN',horizontal=TRUE)
